{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_NWRMSLE(predicted_l, actual_l, weights_l):\n",
    "    p_ = np.array(predicted_l)\n",
    "    a_ = np.array(actual_l)\n",
    "    ret_val = np.sqrt(np.sum((weights_l*np.log(p_+1)-np.log(a_+1))**2)/sum(weights_l))\n",
    "    return ret_val\n",
    "\n",
    "def to_log(arr):\n",
    "    arr_ = np.array(arr)\n",
    "    return np.log(arr_+1)\n",
    "\n",
    "def from_log(arr):\n",
    "    arr_ = np.array(arr)\n",
    "    return np.exp(arr_)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = '/home/as/Desktop/machine learning/___kaggle/CorporacionFavorita'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(root_path+'/data/test.csv', parse_dates=['date'])#,index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing store:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing store:  2\n",
      "processing store:  3\n",
      "processing store:  4\n",
      "processing store:  5\n",
      "processing store:  6\n",
      "processing store:  7\n",
      "processing store:  8\n",
      "processing store:  9\n",
      "processing store:  10\n",
      "processing store:  11\n",
      "processing store:  12\n",
      "processing store:  13\n",
      "processing store:  14\n",
      "processing store:  15\n",
      "processing store:  16\n",
      "processing store:  17\n",
      "processing store:  18\n",
      "processing store:  19\n",
      "processing store:  20\n",
      "processing store:  21\n",
      "processing store:  22\n",
      "processing store:  23\n",
      "processing store:  24\n",
      "processing store:  25\n",
      "processing store:  26\n",
      "processing store:  27\n",
      "processing store:  28\n",
      "processing store:  29\n",
      "processing store:  30\n",
      "processing store:  31\n",
      "processing store:  32\n",
      "processing store:  33\n",
      "processing store:  34\n",
      "processing store:  35\n",
      "processing store:  36\n",
      "processing store:  37\n",
      "processing store:  38\n",
      "processing store:  39\n",
      "processing store:  40\n",
      "processing store:  41\n",
      "processing store:  42\n",
      "processing store:  43\n",
      "processing store:  44\n",
      "processing store:  45\n",
      "processing store:  46\n",
      "processing store:  47\n",
      "processing store:  48\n",
      "processing store:  49\n",
      "processing store:  50\n",
      "processing store:  51\n",
      "processing store:  52\n",
      "processing store:  53\n",
      "processing store:  54\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "test_df.loc[:,'unit_sales'] = 0\n",
    "scores_ = []\n",
    "\n",
    "for store_id in range(1,55):\n",
    "    print('processing store: ',store_id)\n",
    "        #read store data\n",
    "    dtypes = {'id':'uint32', 'item_nbr':'int32', 'store_nbr':'int8', 'unit_sales':'float32'}\n",
    "    td_s_exp = pd.read_csv(root_path+'/data/train_slices3/train_data_store'+str(store_id)+'.csv', \n",
    "                           dtype=dtypes,parse_dates=['date'])\n",
    "    #td_s_exp.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "    ws = td_s_exp.loc[(td_s_exp['date']>='2016-08-01')]\n",
    "    del td_s_exp\n",
    "        #calculate mean sales\n",
    "    ws.loc[ws['unit_sales']<0,['unit_sales']] = 0\n",
    "    ws['dayofweek'] = ws['date'].dt.dayofweek\n",
    "    ws['unit_sales'] = ws['unit_sales'].apply(pd.np.log1p) #converting to x to log(1+x)\n",
    "    ma_dw = ws[['item_nbr','dayofweek','unit_sales']].groupby(['item_nbr','dayofweek'])['unit_sales'].mean()\\\n",
    "                                                       .to_frame('madw').reset_index()\n",
    "    ma_wk = ma_dw[['item_nbr','madw']].groupby(['item_nbr'])['madw'].mean().to_frame('mawk').reset_index()\n",
    "    \n",
    "    u_dates = ws.date.unique()\n",
    "    u_items = ws.item_nbr.unique()\n",
    "    ws.set_index(['date', 'item_nbr'], inplace=True)\n",
    "    ws = ws.reindex(pd.MultiIndex.from_product((u_dates, u_items),names=['date','item_nbr'])).reset_index()\n",
    "    del u_dates, u_items\n",
    "    ws.loc[:, 'unit_sales'].fillna(0, inplace=True) # fill NaNs\n",
    "    \n",
    "    lastdate = ws.iloc[ws.shape[0]-1].date\n",
    "    ma_is = ws[['item_nbr','unit_sales']].groupby(['item_nbr'])['unit_sales'].mean().to_frame('mais')\n",
    "    for i in [112,56,28,14,7,3,1]:\n",
    "        tmp = ws[ws.date>lastdate-timedelta(int(i))]\n",
    "        tmpg = tmp.groupby(['item_nbr'])['unit_sales'].mean().to_frame('mais'+str(i))\n",
    "        ma_is = ma_is.join(tmpg, how='left')\n",
    "    del tmp,tmpg,ws\n",
    "\n",
    "    ma_is['mais']=ma_is.median(axis=1)\n",
    "    ma_is.reset_index(inplace=True)\n",
    "    ma_is.drop(list(ma_is.columns.values)[2:],1,inplace=True) #was [3:]\n",
    "\n",
    "            #get test slice\n",
    "    #ws_t = pd.read_csv(root_path+'/data/test_slices/test_slice_'+str(store_id)+'.csv',index_col='Unnamed: 0')\n",
    "    #ws_t.drop(['outs'+str(sn_) for sn_ in range(1,55,1)],axis=1,inplace=True)\n",
    "    #ws_t.drop(['int'+str(sn_) for sn_ in range(1,55,1) if sn_!=store_id+100],axis=1,inplace=True)\n",
    "    ws_t = test_df[test_df['store_nbr']==store_id]\n",
    "    ws_t['dayofweek'] = ws_t['date'].dt.dayofweek\n",
    "    ws_t = pd.merge(ws_t, ma_is, how='left', on=['item_nbr'])\n",
    "    ws_t = pd.merge(ws_t, ma_wk, how='left', on=['item_nbr'])\n",
    "    ws_t = pd.merge(ws_t, ma_dw, how='left', on=['item_nbr','dayofweek'])\n",
    "    del ma_is, ma_wk, ma_dw\n",
    "            #build forecast\n",
    "    ws_t['unit_sales'] = ws_t.mais \n",
    "    pos_idx = ws_t['mawk'] > 0\n",
    "    test_pos = ws_t.loc[pos_idx]\n",
    "    ws_t.loc[pos_idx, ['unit_sales']] = test_pos['mais'] * test_pos['madw'] / test_pos['mawk']\n",
    "    ws_t.loc[:,\"unit_sales\"].fillna(0, inplace=True)\n",
    "    ws_t['unit_sales'] = ws_t['unit_sales'].apply(pd.np.expm1) # restoring \n",
    "    ws_t.loc[ws_t['onpromotion']==True, ['unit_sales']] *= 1.5\n",
    "    ws_t.loc[:,\"unit_sales\"].fillna(0, inplace=True)\n",
    "    \n",
    "    test_df.loc[test_df['store_nbr']==store_id,['unit_sales']] = np.array(ws_t['unit_sales'])\n",
    "    del ws_t\n",
    "\n",
    "print('done')\n",
    "#test_df.to_csv(root_path+'/data/_test_df_processed_20171212.csv', float_format='%.3f',compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission to kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df[['id','unit_sales']].to_csv(root_path+'/data/_submission.csv.gz',header=True,index=False,float_format='%.3f',\n",
    "                                   compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3.370464e+06\n",
       "mean     3.272853e+00\n",
       "std      1.104077e+01\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      8.078568e-01\n",
       "75%      2.895783e+00\n",
       "max      2.554157e+03\n",
       "Name: unit_sales, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['unit_sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppcsv = pd.read_csv(root_path+'/data/ma8dwof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['unit_sales2'] = ppcsv['unit_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['unit_sales'] = np.round(test_df['unit_sales'],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
